{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RizulThakur001/Med.ipynb/blob/main/Med.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McikxnoUbEO0"
      },
      "source": [
        "# ***Genetic Algorithm***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yLCxc3qzqqC"
      },
      "source": [
        "# **Decision Tree**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ0ECVyZzhyj"
      },
      "source": [
        "# **Entropy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXyKiHf9zQUB"
      },
      "source": [
        "**Random state=42,test=0.2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoNxOeNTzB6K",
        "outputId": "98422e19-e9a2-4f1e-a83f-94bca6f6388e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 3}\n",
            "Test accuracy: 0.9206349206349206\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "def generate_population(size):\n",
        "  \"\"\"Generates a population of decision tree hyperparameters.\"\"\"\n",
        "  return [\n",
        "      {\n",
        "          'criterion': random.choice([ 'entropy']),\n",
        "          'max_depth': random.randint(1, 10),\n",
        "          'min_samples_split': random.randint(2, 20),\n",
        "          'min_samples_leaf': random.randint(1, 10)\n",
        "      }\n",
        "      for _ in range(size)\n",
        "  ]\n",
        "\n",
        "def fitness(individual, x_train, x_test, y_train, y_test):\n",
        "  \"\"\"Calculates the fitness of a decision tree with given hyperparameters.\"\"\"\n",
        "  clf = DecisionTreeClassifier(**individual)\n",
        "  clf.fit(x_train, y_train)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def selection(population, fitnesses):\n",
        "  \"\"\"Selects the fittest individuals (hyperparameter sets).\"\"\"\n",
        "  return random.choices(population, weights=fitnesses,k=2)\n",
        "\n",
        "def crossover(parent1, parent2):\n",
        "  \"\"\"Performs crossover between two sets of hyperparameters.\"\"\"\n",
        "  child = {}\n",
        "  for key in parent1.keys():\n",
        "    child[key] = random.choice([parent1[key], parent2[key]])\n",
        "  return child\n",
        "\n",
        "def mutation(individual, mutation_rate):\n",
        "  \"\"\"Introduces mutations in hyperparameters.\"\"\"\n",
        "  for key in individual.keys():\n",
        "    if random.random() < mutation_rate:\n",
        "      if key == 'criterion':\n",
        "        individual[key] = random.choice([ 'entropy'])\n",
        "      elif key == 'max_depth':\n",
        "        individual[key] = random.randint(1, 10)\n",
        "      elif key == 'min_samples_split':\n",
        "        individual[key] = random.randint(2, 20)\n",
        "      elif key == 'min_samples_leaf':\n",
        "        individual[key] = random.randint(1, 10)\n",
        "  return individual\n",
        "\n",
        "# Load custom dataset\n",
        "df=pd.read_csv('/content/drive/MyDrive/new_data.csv')\n",
        "df\n",
        "x=df.iloc[:, 0:-1].values\n",
        "y=df.iloc[:, -1].values\n",
        "#split the data to train and test the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=57)\n",
        "from sklearn import tree\n",
        "classifier=tree.DecisionTreeClassifier()\n",
        "classifier.fit(x_train,y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "\n",
        "population_size = 80\n",
        "mutation_rate = 0.9\n",
        "\n",
        "population = generate_population(population_size)\n",
        "\n",
        "for generation in range(50):\n",
        "  fitnesses = [fitness(individual, x_train, x_test, y_train, y_test) for individual in population]\n",
        "\n",
        "  parent1, parent2 = selection(population, fitnesses)\n",
        "  child = crossover(parent1, parent2)\n",
        "  child = mutation(child, mutation_rate)\n",
        "\n",
        "  population[-1] = child\n",
        "best_individual = max(population, key=lambda individual: fitness(individual, x_train, x_test, y_train, y_test))\n",
        "\n",
        "best_clf = DecisionTreeClassifier(**best_individual)\n",
        "best_clf.fit(x_train, y_train)\n",
        "y_pred = best_clf.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best hyperparameters:\", best_individual)\n",
        "print(\"Test accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV1syHqRz18Y"
      },
      "source": [
        "# ***GINI***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU8-vfFn00Fq"
      },
      "source": [
        "**random state=42,test=0.2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxMnxmZS0dY0",
        "outputId": "91b093c2-0961-4074-a605-fa3a341d6a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters: {'criterion': 'gini', 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 1}\n",
            "Test accuracy: 0.9365079365079365\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "def generate_population(size):\n",
        "  \"\"\"Generates a population of decision tree hyperparameters.\"\"\"\n",
        "  return [\n",
        "      {\n",
        "          'criterion': random.choice(['gini', 'entropy']),\n",
        "          'max_depth': random.randint(1, 10),\n",
        "          'min_samples_split': random.randint(2, 20),\n",
        "          'min_samples_leaf': random.randint(1, 10)\n",
        "      }\n",
        "      for _ in range(size)\n",
        "  ]\n",
        "\n",
        "def fitness(individual, x_train, x_test, y_train, y_test):\n",
        "  \"\"\"Calculates the fitness of a decision tree with given hyperparameters.\"\"\"\n",
        "  clf = DecisionTreeClassifier(**individual)\n",
        "  clf.fit(x_train, y_train)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def selection(population, fitnesses):\n",
        "  \"\"\"Selects the fittest individuals (hyperparameter sets).\"\"\"\n",
        "  return random.choices(population, weights=fitnesses,k=2)\n",
        "\n",
        "def crossover(parent1, parent2):\n",
        "  \"\"\"Performs crossover between two sets of hyperparameters.\"\"\"\n",
        "  child = {}\n",
        "  for key in parent1.keys():\n",
        "    child[key] = random.choice([parent1[key], parent2[key]])\n",
        "  return child\n",
        "\n",
        "def mutation(individual, mutation_rate):\n",
        "  \"\"\"Introduces mutations in hyperparameters.\"\"\"\n",
        "  for key in individual.keys():\n",
        "    if random.random() < mutation_rate:\n",
        "      if key == 'criterion':\n",
        "        individual[key] = random.choice(['gini', 'entropy'])\n",
        "      elif key == 'max_depth':\n",
        "        individual[key] = random.randint(1, 10)\n",
        "      elif key == 'min_samples_split':\n",
        "        individual[key] = random.randint(2, 20)\n",
        "      elif key == 'min_samples_leaf':\n",
        "        individual[key] = random.randint(1, 10)\n",
        "  return individual\n",
        "\n",
        "# Load custom dataset\n",
        "df=pd.read_csv('/content/drive/MyDrive/new_data.csv')\n",
        "df\n",
        "x=df.iloc[:, 0:-1].values\n",
        "y=df.iloc[:, -1].values\n",
        "le=LabelEncoder()\n",
        "y=le.fit_transform(y)\n",
        "#split the data to train and test the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n",
        "from sklearn import tree\n",
        "classifier=tree.DecisionTreeClassifier()\n",
        "classifier.fit(x,y)\n",
        "y_pred = classifier.predict(x)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y,y_pred)\n",
        "\n",
        "population_size = 90\n",
        "mutation_rate = 0.5\n",
        "\n",
        "population = generate_population(population_size)\n",
        "\n",
        "for generation in range(50):\n",
        "  fitnesses = [fitness(individual, x_train, x_test, y_train, y_test) for individual in population]\n",
        "\n",
        "  parent1, parent2 = selection(population, fitnesses)\n",
        "  child = crossover(parent1, parent2)\n",
        "  child = mutation(child, mutation_rate)\n",
        "\n",
        "  population[-1] = child\n",
        "\n",
        "best_individual = max(population, key=lambda individual: fitness(individual, x_train, x_test, y_train, y_test))\n",
        "\n",
        "best_clf = DecisionTreeClassifier(**best_individual)\n",
        "best_clf.fit(x_train, y_train)\n",
        "y_pred = best_clf.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best hyperparameters:\", best_individual)\n",
        "print(\"Test accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyYR2xf13VgL"
      },
      "source": [
        "# ***Random Forest***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yHMBbP16auI"
      },
      "source": [
        "**random state=42,test=0.2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFes45Ql4DPZ",
        "outputId": "5db35522-b717-48ee-df37-e31700b71c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized accuracy on testing set: 0.9126984126984127\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.optimize import differential_evolution\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/new_data.csv')\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = df.drop('RiskLevel', axis=1)\n",
        "y = df['RiskLevel']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the random forest model\n",
        "def random_forest_model(params):\n",
        "    clf = RandomForestClassifier(n_estimators=int(params[0]), max_depth=int(params[1]), min_samples_split=int(params[2]), min_samples_leaf=int(params[3]))\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_train)\n",
        "    accuracy = accuracy_score(y_train, y_pred)\n",
        "    return -accuracy  # Negative accuracy as the fitness function\n",
        "\n",
        "# Define the bounds for the genetic algorithm\n",
        "bounds = [(10, 100), (1, 10), (2, 10), (1, 10)]  # n_estimators, max_depth, min_samples_split, min_samples_leaf\n",
        "\n",
        "# Run the genetic algorithm\n",
        "result = differential_evolution(random_forest_model, bounds, maxiter=100, popsize=50)\n",
        "\n",
        "# Get the optimized parameters\n",
        "opt_params = result.x\n",
        "\n",
        "# Create the random forest model with the optimized parameters\n",
        "clf_opt = RandomForestClassifier(n_estimators=int(opt_params[0]), max_depth=int(opt_params[1]), min_samples_split=int(opt_params[2]), min_samples_leaf=int(opt_params[3]))\n",
        "clf_opt.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the optimized model on the testing set\n",
        "y_pred = clf_opt.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Optimized accuracy on testing set:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn1OSsvA9OZA"
      },
      "source": [
        "# ***SVM***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jy5DZvNGO2Y"
      },
      "source": [
        "**Random state=42,test=0.2**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sE7NPUn31eC",
        "outputId": "647673e5-9e88-4189-9010-0cc613630784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deap\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.25.2)\n",
            "Installing collected packages: deap\n",
            "Successfully installed deap-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from deap import base, creator, tools, algorithms\n",
        "\n",
        "# Load the Iris dataset\n",
        "data=pd.read_csv('/content/drive/MyDrive/new_data.csv')\n",
        "X=data.drop('RiskLevel',axis=1)\n",
        "y=data['RiskLevel']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "# Define evaluation function\n",
        "def evaluate_svm(individual):\n",
        "    C = individual[0]\n",
        "    gamma = individual[1]\n",
        "    model = SVC(C=C, gamma=gamma, kernel='rbf')\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "    return scores.mean(),\n",
        "\n",
        "# Set up genetic algorithm\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_float\", np.random.uniform, 0.1, 100)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=2)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "toolbox.register(\"mate\", tools.cxBlend, alpha=0.1)\n",
        "toolbox.register(\"mutate\", tools.mutPolynomialBounded, low=0.1, up=100, eta=1.0, indpb=0.2)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"evaluate\", evaluate_svm)\n",
        "\n",
        "population = toolbox.population(n=90)\n",
        "# Genetic Algorithm parameters\n",
        "NGEN = 40\n",
        "CXPB = 0.5\n",
        "MUTPB = 0.5\n",
        "\n",
        "# Run Genetic Algorithm\n",
        "for gen in range(NGEN):\n",
        "    offspring = algorithms.varAnd(population, toolbox, cxpb=CXPB, mutpb=MUTPB)\n",
        "    fits = list(map(toolbox.evaluate, offspring))\n",
        "\n",
        "    for fit, ind in zip(fits, offspring):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    population = toolbox.select(offspring, k=len(population))\n",
        "\n",
        "best_individual = tools.selBest(population, k=1)[0]\n",
        "print(f'Best individual: {best_individual}')\n",
        "print(f'Best fitness: {best_individual.fitness.values[0]}')\n",
        "\n",
        "# Train the final SVM model with the best hyperparameters\n",
        "best_C = best_individual[0]\n",
        "best_gamma = best_individual[1]\n",
        "svm_model = SVC(C=best_C, gamma=best_gamma, kernel='rbf')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDBoIYYJ-LIZ",
        "outputId": "8e49444d-ddac-47ae-9caf-7032805908cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best individual: [74.16402612936587, 0.1383701158452107]\n",
            "Best fitness: 0.9265742574257425\n",
            "[[48  0  1]\n",
            " [ 0 32  2]\n",
            " [ 2  5 36]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   high risk       0.96      0.98      0.97        49\n",
            "    low risk       0.86      0.94      0.90        34\n",
            "    mid risk       0.92      0.84      0.88        43\n",
            "\n",
            "    accuracy                           0.92       126\n",
            "   macro avg       0.92      0.92      0.92       126\n",
            "weighted avg       0.92      0.92      0.92       126\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7ipoZjpGkw4"
      },
      "source": [
        "# ***Naive Bayes***"
      ]
    },
    {
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from deap import base, creator, tools, algorithms\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Load the iris dataset\n",
        "df=pd.read_csv('/content/drive/MyDrive/new_data.csv')\n",
        "X = df.drop('RiskLevel',axis=1)\n",
        "y = df['RiskLevel']\n",
        "# Encode the target variable as a numerical label\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "#set into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate_nb(individual):\n",
        "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
        "    if not selected_features:\n",
        "        return 0,  # Return 0 if no features are selected\n",
        "    # Use .iloc to select columns by integer position\n",
        "    X_train_selected = X_train.iloc[:, selected_features]\n",
        "    nb_model = GaussianNB()\n",
        "    scores = cross_val_score(nb_model, X_train_selected, y_train, cv=5)\n",
        "    return scores.mean(),\n",
        "\n",
        "# Set up genetic algorithm\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_bool\", np.random.randint, 2)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=X.shape[1])\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"evaluate\", evaluate_nb)\n",
        "\n",
        "population = toolbox.population(n=30)\n",
        "\n",
        "# Genetic Algorithm parameters\n",
        "NGEN = 40\n",
        "CXPB = 0.5\n",
        "MUTPB = 0.2\n",
        "\n",
        "# Run Genetic Algorithm\n",
        "for gen in range(NGEN):\n",
        "    offspring = algorithms.varAnd(population, toolbox, cxpb=CXPB, mutpb=MUTPB)\n",
        "    fits = list(map(toolbox.evaluate, offspring))\n",
        "\n",
        "    for fit, ind in zip(fits, offspring):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    population = toolbox.select(offspring, k=len(population))\n",
        "\n",
        "best_individual = tools.selBest(population, k=1)[0]\n",
        "print(f'Best individual: {best_individual}')\n",
        "print(f'Best fitness: {best_individual.fitness.values[0]}')\n",
        "\n",
        "# Get the best selected features\n",
        "selected_features = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
        "# Use .iloc to select columns by integer position\n",
        "X_train_selected = X_train.iloc[:, selected_features]\n",
        "X_test_selected = X_test.iloc[:, selected_features]\n",
        "\n",
        "# Train the final Naive Bayes model with the best selected features\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = nb_model.predict(X_test_selected)\n",
        "\n",
        "# Evaluate the model\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbxJ2NdhTCSZ",
        "outputId": "474e88c8-6935-49d2-82bf-dcf7bc50a0b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best individual: [0, 0, 1, 1, 1, 0, 0, 1, 0]\n",
            "Best fitness: 0.8708120531154238\n",
            "[[63  2  7]\n",
            " [ 0 49  8]\n",
            " [ 2  4 54]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.88      0.92        72\n",
            "           1       0.89      0.86      0.88        57\n",
            "           2       0.78      0.90      0.84        60\n",
            "\n",
            "    accuracy                           0.88       189\n",
            "   macro avg       0.88      0.88      0.88       189\n",
            "weighted avg       0.89      0.88      0.88       189\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGNvvG_veuyv",
        "outputId": "bd7bd79e-983c-4fc1-e6fe-e742e5caecfc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best individual is [0, 0, 1, 1, 1, 0, 0, 1, 0]\n",
            "with fitness: 0.8857142857142857\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from deap import base, creator, tools, algorithms\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the iris dataset\n",
        "df=pd.read_csv('/content/drive/MyDrive/new_data.csv')\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Encode the target variable as a numerical label\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Define a fitness function\n",
        "def evaluate(individual):\n",
        "    # Decode the individual (chromosome) into feature mask\n",
        "    mask = np.array(individual, dtype=bool)\n",
        "\n",
        "    if np.sum(mask) == 0:  # Avoid selecting zero features\n",
        "        return 0.0, # Return a tuple for consistency\n",
        "\n",
        "    # Train Naive Bayes classifier on the selected features\n",
        "    clf = GaussianNB()\n",
        "    clf.fit(X.iloc[:, mask], y)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = clf.score(X.iloc[:, mask], y)\n",
        "\n",
        "    return accuracy, # Return a tuple for consistency\n",
        "\n",
        "# Define the genetic algorithm components\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "# Initialize toolbox\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=X.shape[1])\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"evaluate\", evaluate)\n",
        "\n",
        "# Function to run the genetic algorithm\n",
        "def main():\n",
        "    # Initialize population\n",
        "    population = toolbox.population(n=50)\n",
        "\n",
        "    # Set up the algorithms\n",
        "    NGEN = 20\n",
        "    CXPB, MUTPB = 0.5, 0.2\n",
        "\n",
        "    # Run the genetic algorithm\n",
        "    for gen in range(NGEN):\n",
        "        offspring = algorithms.varAnd(population, toolbox, cxpb=CXPB, mutpb=MUTPB)\n",
        "        fits = list(map(toolbox.evaluate, offspring))\n",
        "\n",
        "        for fit, ind in zip(fits, offspring):\n",
        "            ind.fitness.values = fit # Assign the fitness value as a tuple\n",
        "\n",
        "        population = toolbox.select(offspring, k=len(population))\n",
        "\n",
        "    # Get the best individual\n",
        "    best_ind = tools.selBest(population, k=1)[0]\n",
        "    print(f'Best individual is {best_ind}')\n",
        "    print(f'with fitness: {best_ind.fitness.values[0]}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK7ITK8pqIMU"
      },
      "source": [
        "# ***knn***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2LoGIWB2Rk2"
      },
      "source": [
        "**Rnadom state=42 and test =0.2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVGmBGIzm4vw",
        "outputId": "9f8bb847-7967-4587-f69b-364c4681108d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation 0: Best Fitness = 0.8650793650793651\n",
            "Generation 1: Best Fitness = 0.8333333333333334\n",
            "Generation 2: Best Fitness = 0.8333333333333334\n",
            "Generation 3: Best Fitness = 0.8571428571428571\n",
            "Generation 4: Best Fitness = 0.8571428571428571\n",
            "Generation 5: Best Fitness = 0.8412698412698413\n",
            "Generation 6: Best Fitness = 0.8412698412698413\n",
            "Generation 7: Best Fitness = 0.8412698412698413\n",
            "Generation 8: Best Fitness = 0.8492063492063492\n",
            "Generation 9: Best Fitness = 0.873015873015873\n",
            "Generation 10: Best Fitness = 0.873015873015873\n",
            "Generation 11: Best Fitness = 0.873015873015873\n",
            "Generation 12: Best Fitness = 0.873015873015873\n",
            "Generation 13: Best Fitness = 0.873015873015873\n",
            "Generation 14: Best Fitness = 0.873015873015873\n",
            "Generation 15: Best Fitness = 0.873015873015873\n",
            "Generation 16: Best Fitness = 0.873015873015873\n",
            "Generation 17: Best Fitness = 0.873015873015873\n",
            "Generation 18: Best Fitness = 0.873015873015873\n",
            "Generation 19: Best Fitness = 0.873015873015873\n",
            "Generation 20: Best Fitness = 0.873015873015873\n",
            "Generation 21: Best Fitness = 0.873015873015873\n",
            "Generation 22: Best Fitness = 0.8333333333333334\n",
            "Generation 23: Best Fitness = 0.873015873015873\n",
            "Generation 24: Best Fitness = 0.873015873015873\n",
            "Generation 25: Best Fitness = 0.873015873015873\n",
            "Generation 26: Best Fitness = 0.873015873015873\n",
            "Generation 27: Best Fitness = 0.8333333333333334\n",
            "Generation 28: Best Fitness = 0.8333333333333334\n",
            "Generation 29: Best Fitness = 0.8571428571428571\n",
            "Generation 30: Best Fitness = 0.8571428571428571\n",
            "Generation 31: Best Fitness = 0.8333333333333334\n",
            "Generation 32: Best Fitness = 0.8333333333333334\n",
            "Generation 33: Best Fitness = 0.8333333333333334\n",
            "Generation 34: Best Fitness = 0.8333333333333334\n",
            "Generation 35: Best Fitness = 0.8333333333333334\n",
            "Generation 36: Best Fitness = 0.8333333333333334\n",
            "Generation 37: Best Fitness = 0.8174603174603174\n",
            "Generation 38: Best Fitness = 0.8174603174603174\n",
            "Generation 39: Best Fitness = 0.8174603174603174\n",
            "Generation 40: Best Fitness = 0.8174603174603174\n",
            "Generation 41: Best Fitness = 0.8492063492063492\n",
            "Generation 42: Best Fitness = 0.8492063492063492\n",
            "Generation 43: Best Fitness = 0.8492063492063492\n",
            "Generation 44: Best Fitness = 0.8492063492063492\n",
            "Generation 45: Best Fitness = 0.8571428571428571\n",
            "Generation 46: Best Fitness = 0.8571428571428571\n",
            "Generation 47: Best Fitness = 0.8571428571428571\n",
            "Generation 48: Best Fitness = 0.8571428571428571\n",
            "Generation 49: Best Fitness = 0.8492063492063492\n",
            "Best chromosome: [1 1 1 1 1 1 1 1 1]\n",
            "Selected features: [0 1 2 3 4 5 6 7 8]\n",
            "Final accuracy with selected features: 0.8492063492063492\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Load the dataset\n",
        "df=pd.read_csv('/content/drive/MyDrive/new_data.csv')\n",
        "X=df.drop('RiskLevel',axis=1)\n",
        "y=df['RiskLevel']\n",
        "# Genetic Algorithm parameters\n",
        "population_size = 20\n",
        "num_generations = 50\n",
        "mutation_rate = 0.01\n",
        "num_features = X.shape[1]\n",
        "k_neighbors = 3  # Number of neighbors for k-NN\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize population\n",
        "def initialize_population(pop_size, num_features):\n",
        "    return np.random.randint(2, size=(pop_size, num_features))\n",
        "\n",
        "# Fitness function\n",
        "def fitness(chromosome, X_train, y_train, X_test, y_test): # Added X_test and y_test\n",
        "    selected_features = np.where(chromosome == 1)[0]\n",
        "    if len(selected_features) == 0:\n",
        "        return 0 # Return 0 if no features are selected\n",
        "\n",
        "    X_train_selected = X_train.iloc[:, selected_features] # Use iloc to select from DataFrame\n",
        "    X_test_selected = X_test.iloc[:, selected_features]   # Use iloc to select from DataFrame\n",
        "    model = KNeighborsClassifier(n_neighbors=k_neighbors)\n",
        "    model.fit(X_train_selected, y_train)\n",
        "    y_pred = model.predict(X_test_selected) # Predict on the subset of X_test\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Selection\n",
        "def select_parents(population, fitnesses):\n",
        "    parents = random.choices(population, weights=fitnesses, k=len(population))\n",
        "    return np.array(parents)\n",
        "\n",
        "# Crossover\n",
        "def crossover(parent1, parent2):\n",
        "    crossover_point = random.randint(1, num_features - 1)\n",
        "    child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))\n",
        "    child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))\n",
        "    return child1, child2\n",
        "\n",
        "# Mutation\n",
        "def mutate(chromosome, mutation_rate):\n",
        "    for i in range(len(chromosome)):\n",
        "        if random.random() < mutation_rate:\n",
        "            chromosome[i] = 1 - chromosome[i]\n",
        "\n",
        "# Main Genetic Algorithm loop\n",
        "population = initialize_population(population_size, num_features)\n",
        "for generation in range(num_generations):\n",
        "    # Pass X_test and y_test to the fitness function\n",
        "    fitnesses = np.array([fitness(chromosome, X_train, y_train, X_test, y_test) for chromosome in population])\n",
        "    print(f\"Generation {generation}: Best Fitness = {np.max(fitnesses)}\")\n",
        "\n",
        "    new_population = []\n",
        "    parents = select_parents(population, fitnesses)\n",
        "    for i in range(0, len(parents), 2):\n",
        "        parent1, parent2 = parents[i], parents[i+1]\n",
        "        child1, child2 = crossover(parent1, parent2)\n",
        "        mutate(child1, mutation_rate)\n",
        "        mutate(child2, mutation_rate)\n",
        "        new_population.extend([child1, child2])\n",
        "\n",
        "    population = np.array(new_population)\n",
        "\n",
        "# Best solution\n",
        "# Pass X_test and y_test to the fitness function\n",
        "best_chromosome = population[np.argmax([fitness(chromosome, X_train, y_train, X_test, y_test) for chromosome in population])]\n",
        "best_features = np.where(best_chromosome == 1)[0]\n",
        "print(f\"Best chromosome: {best_chromosome}\")\n",
        "print(f\"Selected features: {best_features}\")\n",
        "\n",
        "# Train final model with\n",
        "X_train_selected = X_train.iloc[:, best_features]\n",
        "X_test_selected = X_test.iloc[:, best_features]\n",
        "final_model = KNeighborsClassifier(n_neighbors=k_neighbors)\n",
        "final_model.fit(X_train_selected, y_train)\n",
        "y_pred = final_model.predict(X_test_selected)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Final accuracy with selected features: {final_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***XGB***"
      ],
      "metadata": {
        "id": "cR5ny-uiUWNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random state = 42, test=0.2**"
      ],
      "metadata": {
        "id": "Fz2r1p-tdrSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from deap import base, creator, tools, algorithms\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Load the dataset\n",
        "df=pd.read_csv('/content/drive/MyDrive/new_data.csv')\n",
        "X=df.drop('RiskLevel',axis=1)\n",
        "y=df['RiskLevel']\n",
        "le=LabelEncoder()\n",
        "y=le.fit_transform(y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the fitness function\n",
        "def evaluate(individual):\n",
        "    params = {\n",
        "        'max_depth': int(individual[0]),\n",
        "        'n_estimators': int(individual[1]),\n",
        "        'learning_rate': individual[2],\n",
        "        'subsample': individual[3]\n",
        "    }\n",
        "    model = xgb.XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss')\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')\n",
        "    return scores.mean(),\n",
        "\n",
        "# Setup the genetic algorithm\n",
        "creator.create('FitnessMax', base.Fitness, weights=(1.0,))\n",
        "creator.create('Individual', list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register('attr_int', np.random.randint, 1, 15)  # for max_depth\n",
        "toolbox.register('attr_n_estimators', np.random.randint, 10, 200)  # for n_estimators\n",
        "toolbox.register('attr_float_lr', np.random.uniform, 0.01, 0.3)  # for learning_rate\n",
        "toolbox.register('attr_float_subsample', np.random.uniform, 0.5, 1.0)  # for subsample\n",
        "\n",
        "toolbox.register('individual', tools.initCycle, creator.Individual,\n",
        "                 (toolbox.attr_int, toolbox.attr_n_estimators, toolbox.attr_float_lr, toolbox.attr_float_subsample), n=1)\n",
        "toolbox.register('population', tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "toolbox.register('mate', tools.cxTwoPoint)\n",
        "toolbox.register('mutate', tools.mutPolynomialBounded, low=[1, 10, 0.01, 0.5], up=[15, 200, 0.3, 1.0], indpb=0.2, eta=1.0)\n",
        "toolbox.register('select', tools.selTournament, tournsize=3)\n",
        "toolbox.register('evaluate', evaluate)\n",
        "\n",
        "# Run the genetic algorithm\n",
        "def main():\n",
        "    population = toolbox.population(n=20)\n",
        "    ngen = 40\n",
        "    cxpb = 0.5\n",
        "    mutpb = 0.2\n",
        "\n",
        "    algorithms.eaSimple(population, toolbox, cxpb, mutpb, ngen, stats=None, halloffame=None, verbose=True)\n",
        "\n",
        "    best_individual = tools.selBest(population, k=1)[0]\n",
        "    print('Best Individual:', best_individual)\n",
        "    print('Best Fitness:', best_individual.fitness.values[0])\n",
        "\n",
        "    # Train the final model with the best parameters\n",
        "    best_params = {\n",
        "        'max_depth': int(best_individual[0]),\n",
        "        'n_estimators': int(best_individual[1]),\n",
        "        'learning_rate': best_individual[2],\n",
        "        'subsample': best_individual[3]\n",
        "    }\n",
        "    final_model = xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss')\n",
        "    final_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the final model on the test set\n",
        "    score = final_model.score(X_test, y_test)\n",
        "    print('Test Accuracy:', score)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "o5cydisgUZH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac82994-c5ef-469d-c119-192030655959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gen\tnevals\n",
            "0  \t20    \n",
            "1  \t17    \n",
            "2  \t9     \n",
            "3  \t13    \n",
            "4  \t8     \n",
            "5  \t11    \n",
            "6  \t14    \n",
            "7  \t14    \n",
            "8  \t14    \n",
            "9  \t9     \n",
            "10 \t12    \n",
            "11 \t10    \n",
            "12 \t7     \n",
            "13 \t9     \n",
            "14 \t16    \n",
            "15 \t13    \n",
            "16 \t12    \n",
            "17 \t15    \n",
            "18 \t11    \n",
            "19 \t13    \n",
            "20 \t13    \n",
            "21 \t14    \n",
            "22 \t11    \n",
            "23 \t9     \n",
            "24 \t14    \n",
            "25 \t12    \n",
            "26 \t18    \n",
            "27 \t16    \n",
            "28 \t10    \n",
            "29 \t10    \n",
            "30 \t9     \n",
            "31 \t10    \n",
            "32 \t9     \n",
            "33 \t10    \n",
            "34 \t8     \n",
            "35 \t14    \n",
            "36 \t8     \n",
            "37 \t12    \n",
            "38 \t14    \n",
            "39 \t11    \n",
            "40 \t12    \n",
            "Best Individual: [11, 57, 0.03805623849708419, 0.6906903524559111]\n",
            "Best Fitness: 0.9325396825396827\n",
            "Test Accuracy: 0.9126984126984127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from deap import base, creator, tools, algorithms\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Load the dataset\n",
        "df=pd.read_csv('/content/drive/MyDrive/new_data.csv')\n",
        "X=df.drop('RiskLevel',axis=1)\n",
        "y=df['RiskLevel']\n",
        "le=LabelEncoder()\n",
        "y=le.fit_transform(y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the fitness function\n",
        "def evaluate(individual):\n",
        "    params = {\n",
        "        'max_depth': int(individual[0]),\n",
        "        'n_estimators': int(individual[1]),\n",
        "        'learning_rate': individual[2],\n",
        "        'subsample': individual[3]\n",
        "    }\n",
        "    model = xgb.XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss')\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')\n",
        "    return scores.mean(),\n",
        "\n",
        "# Setup the genetic algorithm\n",
        "creator.create('FitnessMax', base.Fitness, weights=(1.0,))\n",
        "creator.create('Individual', list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register('attr_int', np.random.randint, 1, 15)  # for max_depth\n",
        "toolbox.register('attr_n_estimators', np.random.randint, 10, 200)  # for n_estimators\n",
        "toolbox.register('attr_float_lr', np.random.uniform, 0.01, 0.3)  # for learning_rate\n",
        "toolbox.register('attr_float_subsample', np.random.uniform, 0.5, 1.0)  # for subsample\n",
        "\n",
        "toolbox.register('individual', tools.initCycle, creator.Individual,\n",
        "                 (toolbox.attr_int, toolbox.attr_n_estimators, toolbox.attr_float_lr, toolbox.attr_float_subsample), n=1)\n",
        "toolbox.register('population', tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "toolbox.register('mate', tools.cxTwoPoint)\n",
        "toolbox.register('mutate', tools.mutPolynomialBounded, low=[1, 10, 0.01, 0.5], up=[15, 200, 0.3, 1.0], indpb=0.2, eta=1.0)\n",
        "toolbox.register('select', tools.selTournament, tournsize=3)\n",
        "toolbox.register('evaluate', evaluate)\n",
        "def genetic_algorithm(population_size, generations, mutation_rate, nevals):\n",
        "    best_params = genetic_algorithm(population_size=50, generations=10, mutation_rate=0.1, nevals=1000)\n",
        "# Run the genetic algorithm\n",
        "def main():\n",
        "    population = toolbox.population(n=20)\n",
        "    ngen = 40\n",
        "    cxpb = 0.5\n",
        "    mutpb = 0.2\n",
        "\n",
        "    algorithms.eaSimple(population, toolbox, cxpb, mutpb, ngen, stats=None, halloffame=None, verbose=True)\n",
        "\n",
        "    best_individual = tools.selBest(population, k=1)[0]\n",
        "    print('Best Individual:', best_individual)\n",
        "    print('Best Fitness:', best_individual.fitness.values[0])\n",
        "\n",
        "    # Train the final model with the best parameters\n",
        "    best_params = {\n",
        "        'max_depth': int(best_individual[0]),\n",
        "        'n_estimators': int(best_individual[1]),\n",
        "        'learning_rate': best_individual[2],\n",
        "        'subsample': best_individual[3]\n",
        "    }\n",
        "    final_model = xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss')\n",
        "    final_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the final model on the test set\n",
        "    score = final_model.score(X_test, y_test)\n",
        "    print('Test Accuracy:', score)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q88lg1NiZfD2",
        "outputId": "9de4729e-2a39-4fd1-9771-0a46c5cf1764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
            "/usr/local/lib/python3.10/dist-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
            "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gen\tnevals\n",
            "0  \t20    \n",
            "1  \t11    \n",
            "2  \t13    \n",
            "3  \t9     \n",
            "4  \t13    \n",
            "5  \t15    \n",
            "6  \t11    \n",
            "7  \t13    \n",
            "8  \t10    \n",
            "9  \t16    \n",
            "10 \t14    \n",
            "11 \t12    \n",
            "12 \t10    \n",
            "13 \t12    \n",
            "14 \t9     \n",
            "15 \t11    \n",
            "16 \t11    \n",
            "17 \t6     \n",
            "18 \t12    \n",
            "19 \t16    \n",
            "20 \t8     \n",
            "21 \t7     \n",
            "22 \t16    \n",
            "23 \t13    \n",
            "24 \t14    \n",
            "25 \t12    \n",
            "26 \t14    \n",
            "27 \t15    \n",
            "28 \t11    \n",
            "29 \t15    \n",
            "30 \t12    \n",
            "31 \t9     \n",
            "32 \t10    \n",
            "33 \t9     \n",
            "34 \t13    \n",
            "35 \t14    \n",
            "36 \t14    \n",
            "37 \t14    \n",
            "38 \t10    \n",
            "39 \t12    \n",
            "40 \t11    \n",
            "Best Individual: [13.011285022123843, 37, 0.06864291522596976, 0.8406915013215863]\n",
            "Best Fitness: 0.9365079365079364\n",
            "Test Accuracy: 0.9126984126984127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***THANK YOU***"
      ],
      "metadata": {
        "id": "pmi9FgV5-WeK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pQ0ECVyZzhyj",
        "hV1syHqRz18Y",
        "AyYR2xf13VgL",
        "Jn1OSsvA9OZA"
      ],
      "provenance": [],
      "mount_file_id": "14QtpI72kANQZlt41a8fKqneQdLWwqXm0",
      "authorship_tag": "ABX9TyODqVlHPowO76TRQ788h3Bx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}